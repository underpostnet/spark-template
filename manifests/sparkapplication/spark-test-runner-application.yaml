# This manifest defines a SparkApplication resource for the Kubernetes Operator.
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  # The name of your application instance in Kubernetes.
  name: scala-spark-hello-world-tests
  # The namespace where the Spark Operator is running.
  namespace: default
spec:
  # The type of application (Scala, Python, R, or Java).
  type: Scala
  # The mode can be 'cluster' or 'client'. 'cluster' is standard for production.
  mode: cluster
  # The Spark version. MUST match the version in the Docker image and build.sbt.
  sparkVersion: "3.5.3"
  # The main class that serves as the entry point for your test runner.
  mainClass: com.example.spark.runner.TestRunner
  # The location of your application JAR inside the Docker container.
  # When running in a container, often just the JAR name is sufficient if it's
  # in a standard classpath location. The 'local://' scheme is crucial
  # to tell Spark the file is already in the image.
  mainApplicationFile: "local:///opt/spark/jars/spark-template.jar"

  # IMPORTANT: Replace this with the actual name of your Docker image after
  # building and pushing it to a registry (like Docker Hub, GCR, ECR, etc.).
  image: "localhost/spark-template:latest" # Using the image name directly

  # Explicitly set the pull policy to 'Never'.
  # This is crucial for local Kind development to ensure Kubernetes does not
  # attempt to pull the image from a remote registry and instead uses the
  # image already loaded into the Kind cluster.
  imagePullPolicy: "Never"

  # Restart policy for the Spark driver pod.
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20

  # Configuration for the Spark driver pod.
  driver:
    cores: 1
    memory: "1024m" # e.g., 1 gigabyte
    labels:
      version: 3.5.3
    # A service account with permissions to create and manage executor pods is required.
    serviceAccount: spark

  # Configuration for the Spark executor pods.
  executor:
    instances: 3
    cores: 1
    memory: "1024m"
    labels:
      version: 3.5.3
    # Node affinity to schedule executors on nodes with GPU capabilities,
    # based on NFD labels.
    nodeSelector:
      "nvidia.com/gpu.present": "true" # A common label applied by NVIDIA GPU operator/NFD

  # Add Spark configuration properties to request GPU and ephemeral storage resources.
  # The Spark Operator translates these properties into Kubernetes resource requests.
  sparkConf:
    # Driver GPU configuration
    spark.kubernetes.driver.resource.gpu.vendor: "nvidia.com"
    spark.kubernetes.driver.resource.gpu.amount: "1"
    # Explicitly tell Spark where the GPU discovery script is located for the driver.
    spark.kubernetes.driver.resource.gpu.discoveryScript: "/opt/spark/scripts/getGpusResources.sh"

    # Executor GPU configuration
    spark.kubernetes.executor.resource.gpu.vendor: "nvidia.com"
    spark.kubernetes.executor.resource.gpu.amount: "1"
    # Explicitly tell Spark where the GPU discovery script is located for executors.
    spark.kubernetes.executor.resource.gpu.discoveryScript: "/opt/spark/scripts/getGpusResources.sh"

    # Ephemeral storage configuration
    spark.kubernetes.driver.request.ephemeralStorage: "1Gi"
    spark.kubernetes.driver.limit.ephemeralStorage: "20Gi"
    spark.kubernetes.executor.request.ephemeralStorage: "1Gi"
    spark.kubernetes.executor.limit.ephemeralStorage: "20Gi"
    # Enable the RAPIDS Accelerator plugin
    spark.plugins: "com.nvidia.spark.SQLPlugin"
    # Enable SQL query acceleration by RAPIDS
    spark.rapids.sql.enabled: "true"
    # Optional: Log detailed RAPIDS explanations for debugging and verification
    spark.rapids.sql.explain: "true"
    # Optional: For more verbose logging of RAPIDS activity
    spark.rapids.sql.logLevel: "INFO" # or DEBUG
